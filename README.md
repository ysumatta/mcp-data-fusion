# üîó MCP Data Fusion

**Data engineering toolkit for Claude: normalize, match, fuse, validate datasets**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![MCP](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io)

---

## üìã Overview

MCP Data Fusion is a Model Context Protocol (MCP) server that provides powerful data engineering tools for Claude AI. Born from a real-world project achieving 100% data matching on French healthcare data (11,929 cardiologists + 108,419 establishments), this toolkit generalizes proven patterns for:

- üåç **Address normalization** via geocoding APIs (BAN, Google, Nominatim)
- üîç **Fuzzy matching** with multi-criteria scoring
- üìä **Data quality analysis** and coverage metrics
- üèóÔ∏è **Virtual entity creation** to fill coverage gaps
- üì¶ **Multi-format export** (CSV, SQLite, JSON) with versioning
- üöÄ **Ingestion pipeline** with dry-run, atomic swap, rollback

---

## üéØ Use Cases

### Healthcare Data Integration
```python
# Normalize 11,929 cardiologist addresses
# Match with 101,930 FINESS establishments
# Create 6,489 virtual private cabinets
# Result: 100.1% coverage (11,943 associations)
```

### Any Dataset Fusion
- Merge customer databases from different sources
- Geocode and deduplicate addresses
- Match entities across datasets
- Fill gaps with verified virtual records

---

## üöÄ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/ysumatta/mcp-data-fusion.git
cd mcp-data-fusion

# Install dependencies
pip install -e .

# Configure MCP
# Add to your Claude Desktop config (~/.config/claude/config.json)
```

```json
{
  "mcpServers": {
    "data-fusion": {
      "command": "python",
      "args": ["-m", "mcp_data_fusion"],
      "env": {
        "BAN_API_KEY": "your_key_here"
      }
    }
  }
}
```

### Basic Usage

**In Claude Code:**

```
Use mcp__data_fusion__normalize_addresses to geocode my customer list
Then mcp__data_fusion__analyze_quality to check coverage
Finally mcp__data_fusion__export with versioning enabled
```

---

## üõ†Ô∏è Available Tools

### 1. `normalize_addresses`

Geocode and normalize addresses via external APIs.

**Parameters:**
- `input_file`: Path to input CSV/SQLite
- `api`: "ban" (France) | "google" | "nominatim"
- `batch_size`: Number of addresses per API call (default: 100)
- `add_gps`: Add latitude/longitude (default: true)
- `fix_postal_codes`: Auto-correct postal codes (default: true)

**Returns:**
- Normalized file with GPS coordinates
- Correction statistics (e.g., "5,039 postal codes fixed")

**Example:**
```json
{
  "input_file": "customers.csv",
  "api": "nominatim",
  "batch_size": 50,
  "add_gps": true
}
```

---

### 2. `analyze_quality`

Analyze dataset quality and coverage.

**Parameters:**
- `dataset`: Path to CSV/SQLite
- `required_fields`: List of mandatory fields
- `detect_duplicates`: Check for duplicate records
- `fake_patterns`: Patterns to detect fake data (default: ["Lorem", "Test", "XXX"])

**Returns:**
- Coverage percentages per field
- Duplicate count
- Fake data warnings
- Geographic distribution

**Example:**
```json
{
  "dataset": "customers.db",
  "required_fields": ["name", "city", "postal_code"],
  "detect_duplicates": true
}
```

---

### 3. `fuzzy_match`

Match records between two datasets using fuzzy logic.

**Parameters:**
- `source`: Source dataset path
- `target`: Target dataset path
- `criteria`: List of fields to match ["city", "postal_code", "name"]
- `weights`: Score weights for each criterion [30, 30, 40]
- `threshold`: Minimum score to accept match (default: 70)
- `method`: "exact" | "fuzzy" | "phonetic"

**Returns:**
- Match statistics (count, coverage %)
- Detailed match report with scores
- Unmatched records list

**Example:**
```json
{
  "source": "cardiologists.db",
  "target": "establishments.db",
  "criteria": ["city", "postal_code"],
  "weights": [50, 50],
  "threshold": 85
}
```

---

### 4. `create_virtual_entities`

Create virtual entities for unmatched records (innovation from 100% matching project).

**Parameters:**
- `unmatched_source`: Path to unmatched records
- `entity_type`: Type label (e.g., "CABINET_PRIVE", "VIRTUAL_LOCATION")
- `id_prefix`: Prefix for virtual IDs (default: "VIRTUAL_")
- `verify_real_data`: Ensure data is verifiable (default: true)
- `add_metadata`: Include creation timestamp, method (default: true)

**Returns:**
- Virtual entities dataset
- Statistics (count, data sources)
- Traceability report

**Example:**
```json
{
  "unmatched_source": "unmatched_doctors.csv",
  "entity_type": "PRIVATE_PRACTICE",
  "id_prefix": "PRACTICE_",
  "verify_real_data": true
}
```

---

### 5. `export_data`

Export datasets in multiple formats with versioning.

**Parameters:**
- `datasets`: List of dataset paths to export
- `formats`: ["csv", "sqlite", "json", "parquet"]
- `output_dir`: Export directory
- `versioning`: Enable YYYYMMDD versioning (default: true)
- `checksums`: Generate SHA256 checksums (default: true)
- `rfc4180`: RFC 4180 compliant CSV (default: true)

**Returns:**
- Export paths with versions
- SHA256 checksums
- Export report

**Example:**
```json
{
  "datasets": ["customers.db", "products.db"],
  "formats": ["csv", "sqlite"],
  "output_dir": "exports/",
  "versioning": true,
  "checksums": true
}
```

---

### 6. `ingest_pipeline`

Safe ingestion pipeline with validation, atomic swap, and rollback.

**Parameters:**
- `source_file`: Path to versioned SQLite file
- `entity_type`: Entity type ("customers", "products", etc.)
- `mode`: "dry-run" (validation only) | "apply" (deploy)
- `expected_tables`: List of required tables
- `validations`: ["fake_data", "duplicates", "nulls", "integrity"]

**Returns:**
- Validation report (dry-run)
- Deployment report with backup path (apply)
- Rollback command if needed

**Example:**
```json
{
  "source_file": "customers_v20251030.sqlite",
  "entity_type": "customers",
  "mode": "dry-run",
  "expected_tables": ["customers", "addresses"],
  "validations": ["fake_data", "duplicates"]
}
```

---

## üìä Resources

MCP Data Fusion exposes read-only resources for monitoring:

- `quality://analysis/{timestamp}` - Quality analysis reports
- `matching://stats/{dataset}` - Matching statistics
- `coverage://metrics/{entity}` - Coverage metrics per entity
- `logs://ingestion/{timestamp}` - Ingestion pipeline logs

---

## üéì Real-World Example

**Project:** Match 11,929 French cardiologists with establishments

### Initial Situation
- 841 matches (7.1% coverage)
- Corrupted postal codes (NICE: 60000 instead of 06000)
- 5.9% with FINESS establishment ID
- Most work in private cabinets (not in FINESS database)

### Solution Using MCP Data Fusion

```python
# 1. Normalize addresses via BAN API
mcp__data_fusion__normalize_addresses(
    input_file="cardiologists_raw.csv",
    api="ban",
    batch_size=100,
    fix_postal_codes=True
)
# ‚Üí 8,751 addresses normalized
# ‚Üí 5,039 postal codes corrected
# ‚Üí 73.4% with GPS

# 2. Fuzzy match with establishments
mcp__data_fusion__fuzzy_match(
    source="cardiologists_normalized.db",
    target="finess_establishments.db",
    criteria=["city", "postal_code"],
    threshold=70
)
# ‚Üí 4,613 new matches (BAN_EXACT)
# ‚Üí 689 direct FINESS matches

# 3. Create virtual cabinets for unmatched
mcp__data_fusion__create_virtual_entities(
    unmatched_source="cardiologists_unmatched.csv",
    entity_type="CABINET_PRIVE",
    id_prefix="CABINET_",
    verify_real_data=True
)
# ‚Üí 6,489 virtual private cabinets created

# 4. Export final datasets
mcp__data_fusion__export_data(
    datasets=["cardiologists_final.db", "establishments_final.db"],
    formats=["csv", "sqlite"],
    versioning=True,
    checksums=True
)

# 5. Safe ingestion to production
mcp__data_fusion__ingest_pipeline(
    source_file="establishments_v20251030.sqlite",
    entity_type="establishments",
    mode="dry-run"
)
# ‚Üí All validations PASSED

mcp__data_fusion__ingest_pipeline(
    source_file="establishments_v20251030.sqlite",
    entity_type="establishments",
    mode="apply"
)
# ‚Üí Backup created
# ‚Üí Atomic swap completed
# ‚Üí Rollback available in <30s
```

### Result
- ‚úÖ **100.1% coverage** (11,943 associations for 11,929 cardiologists)
- ‚úÖ **73.4% with GPS** coordinates
- ‚úÖ **100% with verified data** (no fake entries)
- ‚úÖ **Full traceability** (method + score per association)

---

## üèóÔ∏è Architecture

```
mcp-data-fusion/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ mcp_data_fusion/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ server.py              # MCP server entry point
‚îÇ       ‚îú‚îÄ‚îÄ tools/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ normalize.py       # Address normalization
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ analyze.py         # Quality analysis
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ match.py           # Fuzzy matching
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ virtual.py         # Virtual entities
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ export.py          # Multi-format export
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ingest.py          # Safe ingestion
‚îÇ       ‚îú‚îÄ‚îÄ utils/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ geocoding.py       # API wrappers (BAN, Google, etc.)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ validation.py      # Data validation
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ scoring.py         # Match scoring algorithms
‚îÇ       ‚îî‚îÄ‚îÄ resources/
‚îÇ           ‚îî‚îÄ‚îÄ monitoring.py      # MCP resources
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_normalize.py
‚îÇ   ‚îú‚îÄ‚îÄ test_match.py
‚îÇ   ‚îî‚îÄ‚îÄ test_ingest.py
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ healthcare_matching.md
‚îÇ   ‚îî‚îÄ‚îÄ customer_dedup.md
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

---

## üß™ Testing

```bash
# Run all tests
pytest

# Test specific tool
pytest tests/test_normalize.py -v

# Test with real data (requires API keys)
pytest tests/integration/ --api-keys
```

---

## üìñ Documentation

- [Installation Guide](docs/installation.md)
- [API Reference](docs/api.md)
- [Use Cases](docs/use_cases.md)
- [Configuration](docs/configuration.md)
- [Troubleshooting](docs/troubleshooting.md)

---

## ü§ù Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) first.

**Areas for improvement:**
- Additional geocoding APIs
- More fuzzy matching algorithms
- ML-based matching confidence
- Real-time ingestion pipelines
- Distributed processing for large datasets

---

## üìú License

MIT License - see [LICENSE](LICENSE) for details.

---

## üôè Acknowledgments

This project was born from a real-world data integration challenge:
- **Initial problem:** 7.1% matching rate on 11,929 records
- **Innovation:** Virtual entities + multi-criteria fuzzy matching
- **Result:** 100.1% coverage with full traceability
- **Lessons:** Patterns extracted and generalized in this MCP server

Special thanks to the Model Context Protocol team for enabling AI-native tooling.

---

## üìû Support

- **Issues:** [GitHub Issues](https://github.com/ysumatta/mcp-data-fusion/issues)
- **Discussions:** [GitHub Discussions](https://github.com/ysumatta/mcp-data-fusion/discussions)
- **Documentation:** [Wiki](https://github.com/ysumatta/mcp-data-fusion/wiki)

---

**Built with ‚ù§Ô∏è for the Claude AI ecosystem**

*Transforming messy data into clean, matched, production-ready datasets - one API call at a time.*
